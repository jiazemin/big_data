# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# source 
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /var/log/hive/hive.log
a1.sources.r1.shell = /bin/sh -c

#sinks
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoop200:9000/flume/%Y%m%d/%H
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = hive_hdfs_
#是否按照时间滚动文件夹
a1.sinks.k1.hdfs.round = true
#多少时间单位创建一个新的文件夹
a1.sinks.k1.hdfs.roundValue = 1
#重新定义时间单位
a1.sinks.k1.hdfs.roundUnit = hour
#是否使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#积攒多少个Event才flush到HDFS一次
a1.sinks.k1.hdfs.batchSize = 1000
#设置文件类型，可支持压缩
a1.sinks.k1.hdfs.fileType = DataStream
#多久生成一个新的文件
a1.sinks.k1.hdfs.rollInterval = 600
#设置每个文件的滚动大小
a1.sinks.k1.hdfs.rollSize = 134217700
#文件的滚动与Even量t数无关
a1.sinks.k1.hdfs.rollCount = 0
#最小副本数
a1.sinks.k1.hdfs.minBlockReplicas = 1

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
#channels 可以存多少个event
a1.channels.c1.capacity = 1000
#每次可以传输多少个event
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
#sources 与 channel 关联
a1.sources.r1.channels = c1
#sinks 与 channel 关联
a1.sinks.k1.channel = c1