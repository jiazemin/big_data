a1.sources = r1
a1.channels = c1
a1.sinks = k1

a1.sources.r1.type = com.soap.flume.source.TaildirSource
a1.sources.r1.channels = c1
#指定文件读取偏移量存储位置
a1.sources.r1.positionFile = /opt/module/apache-flume-1.6.0-bin/checkpoint1/behavior/taildir_position.json
#source 组可同时监听多个目录
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /opt/app/tomcat1/logs/soap.log
a1.sources.r1.filegroups.f2 = /opt/app/tomcat2/logs/soap.log

#source 监听一个目录
#a1.sources.r1.filegroups = f1
#a1.sources.r1.filegroups.f1 = /opt/app/tomcat1/logs/soap.log

a1.sources.r1.fileHeader = true

#filter 将日志中没有 activeTimeInMs 的过滤掉
#a1.sources.r1.interceptors=i1 i2
#a1.sources.r1.interceptors.i1.type= regex_filter
#a1.sources.r1.interceptors.i1.regex=activeTimeInMs
# excludeEvents = true 包含则拦截
#a1.sources.r1.interceptors.i1.excludeEvents = true

#往event的header中插入关键词为timestamp的时间戳。
#a1.sources.r1.interceptors.i2.type = timestamp
#a1.sources.r1.interceptors.i2.preserveExisting= true


#Flume向kafka发布数据时，发现kafka接收到的数据总是在一个partition中，而我们希望发布来的数据在所有的partition平均分布
a1.sources.r1.interceptors = i2
a1.sources.r1.interceptors.i2.type = org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder
a1.sources.r1.interceptors.i2.headerName = key
a1.sources.r1.interceptors.i2.preserveExisting = false

a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/apache-flume-1.6.0-bin/checkpoint1/behavior_collect
a1.channels.c1.dataDirs = /opt/module/apache-flume-1.6.0-bin/data1/behavior_collect
a1.channels.c1.maxFileSize = 104857600
a1.channels.c1.capacity = 90000000
a1.channels.c1.keep-alive = 60

a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.topic = log_analysis
a1.sinks.k1.brokerList=hadoop200:9092,hadoop201:9092,hadoop202:9092
a1.sinks.k1.requiredAcks = 1
a1.sinks.k1.kafka.producer.type = sync
a1.sinks.k1.batchSize = 10
a1.sinks.k1.channel = c1